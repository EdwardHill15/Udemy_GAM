---
title: "Generalized Linear (GLM) and Additive Modeling (GAM) with R. An Online       Course Presented by Geoffrey S. Hubona"
author: "Edward Hill"
date: "18-12-2021"
output: 
  html_document:
      theme: journal
bibliography: R_GAM.bib
biblio-style: apalike
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

![Generalized Additive Models by Simon N. Wood](https://github.com/EdwardHill15/Udemy_GAM/blob/main/Wood.jpg?raw=true)

![The R Book by Michael Crawley](https://github.com/EdwardHill15/Udemy_GAM/blob/main/Crawley.jpg?raw=true)

# Introduction

## Linear Regression, GLM's and GAMs with R

**Linear Regression**, **GLMs** and **GAMs** with R demonstrates how to use R to extend the basic assumptions and constraints of linear regression to specify, model, and interpret the results of generalized linear (GLMs) and generalized additive (GAMs) models. The course demonstrates the estimation of GLMs and GAMs by working through a series of practical examples from the book Generalized Additive Models: An Introduction with R by Simon N. Wood [@Wood2017]. 

**Linear statistical models have a univariate response** modeled as a linear function of predictor variables and a zero mean random error term. The assumption of linearity is a critical (and limiting) characteristic. 

**Generalized linear models (GLMs) relax this assumption of linearity**. They permit the expected value of the response variable to be a **smoothed** (e.g. non-linear) monotonic function of the linear predictors. **GLMs also relax the assumption that the response variable is normally distributed** by allowing for many distributions (e.g. normal, poisson, binomial, log-linear, etc.). 

**Generalized additive models (GAMs) are extensions of GLMs**. GAMs allow for the estimation of regression coefficients that take the form of **non-parametric smoothers**. Nonparametric smoothers like lowess (locally weighted scatterplot smoothing) fit a smooth curve to data using localized subsets of the data. 

This course provides an overview of modeling GLMs and GAMs using R. GLMs, and especially GAMs, have evolved into standard statistical methodologies of considerable flexibility. The course addresses recent approaches to modeling, estimating and interpreting GAMs. The focus of the course is on **modeling and interpreting GLMs and especially GAMs with R**. Use of the freely available R software illustrates the practicalities of linear, generalized linear, and generalized additive models.

**What you'll learn**

* Understand the assumptions of ordinary least squares (OLS) linear regression.
* Specify, estimate and interpret linear (regression) models using R.
* Understand how the assumptions of OLS regression are modified (relaxed) in order to specify, estimate and interpret generalized linear models (GLMs).
* Specify, estimate and interpret GLMs using R.
* Understand the mechanics and limitations of specifying, estimating and interpreting generalized additive models (GAMs).

**For whom is this course:**

* This course would be useful for anyone involved with linear modeling estimation, including graduate students and/or working professionals in quantitative modeling and data analysis.
* The focus, and majority of content, of this course is on generalized additive modeling. Anyone who wishes to learn how to specify, estimate and interpret GAMs would especially benefit from this course.

## Relevant Literature

1. [@Wood2017]

2. [@Crawley2013]

3. [How much more likely? The Implications for Odds Ratios for Probabilities by Liberman](https://github.com/EdwardHill15/Udemy_GAM/blob/main/Linear%20Regression%20GLMs%20and%20GAMs%20with%20R/other%20documentation/liberman_2005_OR_paper_AJE.PDF)
[@Liberman2005]

## How Old is the Universe

* The big-bang model implies that the universe expands uniformly according to Hubble's law:[^note1] 

$$
y = \beta x
$$
* Where $y$ is the relative velocity of any two galaxies seperated by distance $x$, and $\beta$ is "Hubble's constant". 

* $\beta^{-1}$ is the approximate age of the universe, but $\beta$ is unknown and must be estimated from observations of $x$ and $y$. 

See; @Wood2017, p. 1-9

```{r}
library(gamair) # contains 'hubble'
data(hubble)
hubble
hub.mod <- lm(y~x-1,data=hubble)
summary(hub.mod)
head(hub.mod, 25)
```

### Plot the residuals against the fitted values

```{r}
plot(fitted(hub.mod),residuals(hub.mod),xlab="fitted values",ylab="residuals")
```

### Omit offending points and produce new residual plot
```{r}
hub.mod1 <- lm(y~x-1,data=hubble[-c(3,15),])
summary(hub.mod1)
plot(fitted(hub.mod1),residuals(hub.mod1),xlab="fitted values",ylab="residuals")
```

### Estimate Hubble's Constant

```{r}
hubble.const <- c(coef(hub.mod),coef(hub.mod1))/3.09e19
age <- 1/hubble.const
age
age/(60^2*24*365)
```

### 1.1.3 Adding a distributional assumption

### Testing Hypotheses about \beta

```{r}
cs.hubble <- 163000000
t.stat<-(coef(hub.mod1)-cs.hubble)/summary(hub.mod1)$coefficients[2]
pt(t.stat,df=21)*2
```


### Confidence intervals

```{r}
sigb <- summary(hub.mod1)$coefficients[2]
h.ci<-coef(hub.mod1)+qt(c(0.025,0.975),df=21)*sigb
h.ci
h.ci<-h.ci*60^2*24*365.25/3.09e19 # convert to 1/years
sort(1/h.ci)
```


[^note1]: Easiest way of writing mathematical equation in R Markdown  See Youtube: https://www.youtube.com/watch?v=4I3PCDME5U8)


# References